---
layout: post
title: Dec 2023
abbrlink: 5cb398f1511e45b095fa3e65c118dd62
tags: []
categories:
  - Research
  - Report
date: 1731045583348
updated: 1731915127120
---

**2MR Report**\
Author: Eymeric **Chauchat**\
Version: V1.0 (11 Nov 2024)

## Goal and motivation of the report

This is the first 2-mensual report that I am going to write. That's why I am going to summarize quickly my research goal. For a more in depth overview of the introduction of my research, one's can look [May 2023 Report](/p/22a56c0680934dcdb94881992c682390).
After this introduction and presentation, I will go deeply in each section of my work in the last months.

## Introduction

I will just list the different section and tell weather or not their are really important to read depending on the overview one's want on my research.

- **Research goal :** gives again a summary of my research, it is updated but it hasn't changed a lot. I addition one's can get some insight on less related research goal (called *Life* sub-goals)
- **Life sub goals :** this is clearly an *optionnal* part, I explain the solution I found to the *Life* related sub-goals. I will talk about software and technical solution of this really basics sub-goal
- **Python Xl-sindy library :** I explain my process of creating the python library for my library
- **Reinforcement learning learning :** I think it is the most citation intensive part of the report. I explain what I found abour Reinforcement learning and how it can be linked with my research.
- **RL-Sindy start of work :**

## Research goal

I will present an updated view of my research goal. The goal is to present a global goal and after to divide it into sub-goal.

The main goal of my research is to **Develop a complete control framework using Euler Lagrange SINDy**. Some sub objectives can be listed subsequently :

- **Achieve a complete interpretable control scheme while being data learned**
- **Compare our framework with RL-SINDy**
- **Run it on a real robot**

### Sub-goal

I will list all my sub-goals with some information on why I defined this sub-goals. Sub-goals can be research related *( R )* and some other can be more life related *( L )*.
The life related Sub-goal are more an explanation or where goes my time when I don't produce tangible result.

#### *( R )* Xl-sindy Python library

Started as an idea and became reality. In order to get a clear base code only concerning the xl-Sindy part I wanted to organise correctly my code for the xl-Sindy part.

I did it (ref. [May 2023#code-rebuild](/p/22a56c0680934dcdb94881992c682390#Code-Rebuild)  but after I worked on other part of my project (reinforcement learning) and when I came back at the xl-sindy part, I couldn't understand anything... So in addition with clear code I needed some documentation and some really clear naming convention.

Ultimately, if I do 90% of the work needed to make a library, it means that I can release a true library.

#### *( R )* Get knowledge about Deep Reinforcement Learning

Around June, I tried my first shot at Deep Reinforcement Learning. Some things worked, some things didn't, but the main issue was that I didn't know why... So I wanted to get knowledge about Deep Reinforcement Learning.

#### *( L )* Become git efficient

I wanted to create a python library, so I needed to find a way to be better at git.

#### *( L )* Get a notes management software

Complexity of the project became really high as soon as I went deep into *Deep Reinforcement Learning*. I read books, I did a lot of test on some strange library, I got result that had different type of formating. Every solution of my problems where on forum, lost on StackExchange or at the end of some documentation of library.

I couldn't keep everything as markdown on my different github repo so I needed to find a tool to be able to note everything down

#### *( L )* Learn how to write in an impersonnal scientific tone

As I want to be able to write a paper for February, I need to learn how to write in a scientific impersonnal tone. I learn this by comparing paper, asking AI advice, and trying my best to write in this tone... Unfortunately it takes a lot of time to write a lot of different iteration of same text. I hope that in the following Monthly report I will be able to provide more and more text matching the requirement for scientific paper.
For this month I have focused my attention on formatting correctly my tutorial for making a python library.

### Conference goal

I have set-up the goal to publish for IROS 25 in china. The goal of this first paper on the subject for me is to :

- Compare Xl-Sindy with RL-sindy on Lagrangian-compatible system
- Present the py-xl-sindy python library
- *(Optional)* Show result on a true robot

## Life Sub goals

I will detail the different tool I use to enhance my workflow.

### Zotero Referencing software

I recently started using Zotero to keep track of the paper I read. Zotero is entirely open source software that retrieve citing data of a high number of journal publisher. It compose a library of your document and can be linked efficiently through bibtex or any other citing format. It has been relieving to be able to easily add (through firefox addon), manage and share published paper.

### Joplin note taking app

This app enable a fast and pratical take of note, everything is taken in markdown. The layered structure of notebook enable a high granularity in the sorting of the note.

In the end I nearly never use anymore any text editor when I want to write commands, short tutorial or thought, because I know that I won't loose anything in Joplin if it is written inside of it.

Joplin is totally open source and even the synchronisation server can be run thanks to a docker container.
An active community maintain the code and create a lot of different useful plugin. I will detail some of these plugin in the next sub sections

![Joplin front page](/resources/4111bd7ada4344cda441e10472f1e758.png)

#### Static github.io website

The main plugin for these kind of report is the ability to use Joplin as a Content Management Software for static pages website (like blog). In an automated manner I can synchronise all the notes tagged `blog` and send them to a repositery that will generate through github pages a static website. This is really powerful tool to share devlog, quick draft, tutorial with people.

![Static website from Joplin note](/resources/c9d62c1523e849cfbaa2eb1b3e3f5a19.png)

#### Graphe visualisation and UI improvement

One's can get two plugin that enable backlink (ability to goes back in reference), fast reference searches and graph visualisation to get a nice view of your note architecture.

The main advantages of note taking app instead of wiki engines is the possibility to create floating note (referenced nowhere), this advantages is mitigated with the fact that floating note can be lost in the quantity (and only retrieve by searching), graph visualisation is a great way to keep track of floating note and to understand better the knowledge flow in all the note.

![Graph visualisation of Joplin note](/resources/efb50cc7614245d781a8e1ef2d800354.png)

#### Referencing from bibTex

Joplin can be linked through a Bibtex file in order to quickly cite author. I generate this BibTex automatically thanks to Zotero (and it is update automatically) so I know that I can site quickly whenever I have just added to my Zotero library

### Get better at git

I wanted to increase my skill at git in order to be able to maintain my python library correctly. This has been made at the same time as formatting the library. One's can get a total overview here : [Creating a Python library#summary-of-git-strategy](/p/738f25b6283a408aa2f517964cda0fc5#Summary-of-git-strategy)

## Python Xl-Sindy Library

In order to be able to quickly test my version of xl-Sindy with other Sindy frameword, I needed an abstract way to launch my simulation. The smartest way was to compile everything in a python library. I aim to keep the knowledge I develop through a detailed and well documented library of method (not object oriented) and usage exemple file. The detail of the process can be found here : [Creating a Python library](/p/738f25b6283a408aa2f517964cda0fc5) .

The maintainer repository can be found on git [py-xl-sindy](https://github.com/Eymeric65/py-xl-sindy) and the actual binary are automatically push to PyPi at each release ==here==

## Reinforcement learning learning

The quest of knowloedge for reinforcement learning has started by a bunch of try and error using Reinforcement Learning algorithm. Without giving much detail I will go detail this short quest and my actual knowledge.

### Reinforcement learning framework

During my different test on Reinforcement Learning I had the opportunity to try two different framework :

- **Rlib**
- **CleanRL**

#### Rlib [@liangRLlibAbstractionsDistributed2018](http://dx.doi.org/10.48550%2FarXiv.1712.09381)

First of all I have tried to use Rlib in order to launch some Reinforcement learning experiment. I have been stopped by the complexity of writing a custom environment for Rlib. (Needed to create a custom Gym environment...)
The high abstraction of the reinforcement learning function also caused a lot of confusion. This lead to some time where not a lot of stuff worked. Since Rlib is also used by the RL-sindy team [@zolmanSINDyRLInterpretableEfficient2024](https://scholar.google.com/scholar?q=SINDy-RL%3A%20Interpretable%20and%20Efficient%20Model-Based%20Reinforcement%20Learning%2C%20Zolman).I think I will need to come back to RLib in time, but until then I shifted my focus on CleanRL. [@huangCleanRLHighqualitySinglefile2021](http://dx.doi.org/10.48550%2FarXiv.2111.08819)

#### CleanRL [@huangCleanRLHighqualitySinglefile2021](http://dx.doi.org/10.48550%2FarXiv.2111.08819)

CleanRL is a python library that focus in implementation of state of the art algorithm in one file. In my own opinion it is a must in order to deeply understand what happen inside a Deep Reinforcement Learning algorithm. If I succeed to I would like to perform the comparison of SINDy algorithm inside cleanRL.
In order to install CleanRL (and the so infamous *poetry* lib manager on the laboratory server) I have written some ressources here : [Clean RL Installation](/p/a1f75c7d68c84d8ba8227f2f33b53752)

### Reinforcement Learning theory

In the multiple paper I read about Reinforcement learning, they always directed to a book [@suttonReinforcementLearningIntroduction](https://scholar.google.com/scholar?q=Reinforcement%20Learning%3A%20An%20Introduction%2C%20Sutton). My personnal reading note can be found here [Reinforcement Learning An introduction book](/p/714371adea434a4db72013259193ea24) (really informal). This book has been a key to understand reinforcement learning has a whole.
It covers topic from the beginning of reinforcement learning. With exploration of finite action domain and solution to one state problem until the more complex Policy algorithm (that lead to algorithm like PPO).

I will know dive in some quick explanation of the bases of reinforcement learning. Since I only read the **Tabular method** on this 2MR I will give a little insight of these preliminary method.

- **Greedy, $\epsilon$-greedy, ...**
  Reinforcement learning has in it heart the concept of learning. Learning means that some times you need to make mistake, wether it can be short term mistake (something that can be seen as a mistake but in fact in the long run enable a better outcome), or just mistake that pave the way for the optimal outcome. In reinforcement learning taking the action that maximise the known outcome is called taking a **greedy** action. Always take a greedy action is a good way to maximise the known outcome (by definition we maximise it) but it is really bad to be **greedy** when we want to explore the action domain. That's why it exits a lot of way to choose action and for example the **$\epsilon$-greedy** one state that with a probabilitiy of $\epsilon$ we take a total random action instead of taking a **greedy** one.

- **Dynamic Programming method :**
  This is the base of every Reinforcement learning, a true tabular method. The goal of *Dynamic Programming* is to optimally solve sub-problem in order to grasp the solution of the whole problem. Usually Dynamic programming method will solve every problem possible before finding the solution of our specific problem (choosing every solution path to be certain that our problem has the optimal solution)

- **Monte Carlo method :**
  Monte Carlo method is the first statistical method that appear in the book. The goal is simple : take random (but in a clever way) action in order to in average converge to the optimal solution. This method unlike Dynamic Programming doesn't usually need to solve the whole problem in order to get the solution of ours. The statistical nature of monte carlo enable the algorithm to explore local solution in order to find the global minimum (Soap bubble height algorithm is an awesome intuitive proof for it)

## References

1. Zolman, N., Fasel, U., Kutz, J. N. & Brunton, S. L. SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning. (arXiv, 2024).

2. Huang, S., Dossa, R. F. J., Ye, C. & Braga, J. CleanRL: High-Quality Single-File Implementations of Deep Reinforcement Learning Algorithms. (arXiv, 2021). doi:10.48550/arXiv.2111.08819.

3. Liang, E. et al. RLlib: Abstractions for Distributed Reinforcement Learning. (arXiv, 2018). doi:10.48550/arXiv.1712.09381.

4. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction.
